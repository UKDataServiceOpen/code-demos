{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![UKDS Logo](./images/UKDS_Logos_Col_Grey_300dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Collecting Data I: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Welcome to the <a href=\"https://ukdataservice.ac.uk/\" target=_blank>UK Data Service</a> training series on *New Forms of Data for Social Science Research*. This series guides you through some of the most common and valuable new sources of data available for social science research: data collected from websites, social media platorms, text data, conducting simulations (agent based modelling), to name a few. To help you get to grips with these new forms of data, we provide webinars, interactive notebooks containing live programming code, reading lists and more.\n",
    "\n",
    "* To access training materials for the entire series: <a href=\"https://github.com/UKDataServiceOpen/new-forms-of-data\" target=_blank>[Training Materials]</a>\n",
    "\n",
    "* To keep up to date with upcoming and past training events: <a href=\"https://ukdataservice.ac.uk/news-and-events/events\" target=_blank>[Events]</a>\n",
    "\n",
    "* To get in contact with feedback, ideas or to seek assistance: <a href=\"https://ukdataservice.ac.uk/help.aspx\" target=_blank>[Help]</a>\n",
    "\n",
    "<a href=\"https://www.research.manchester.ac.uk/portal/julia.kasmire.html\" target=_blank>Dr Julia Kasmire</a> and <a href=\"https://www.research.manchester.ac.uk/portal/diarmuid.mcdonnell.html\" target=_blank>Dr Diarmuid McDonnell</a> <br />\n",
    "UK Data Service  <br />\n",
    "University of Manchester <br />\n",
    "May 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods for collecting, cleaning and analysing data are an increasingly important component of a social scientistâ€™s toolkit. Central to engaging in these methods is the ability to write readable and effective code using a programming language.\n",
    "\n",
    "In this training series we demonstrate core programming concepts and methods through the use of social science examples. In particular we focus on four areas of programming/computational social science:\n",
    "1. Introduction to Python.\n",
    "2. Collecting data I: web-scraping. [Focus of this notebook]\n",
    "3. Collecting data II: APIs.\n",
    "4. Setting up your computational environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson - **Collecting data I: web-scraping** - has two aims:\n",
    "1. Demonstrate how to use Python to collect data found on websites.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data collection problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 30-60 minutes\n",
    "* **Pre-requisites**: None, though you may find it useful to work through our <a href=\"https://github.com/UKDataServiceOpen/code-demos/blob/master/code/ukds-intro-to-python-2020-05-06.ipynb\" target=_blank>*Introduction to Python for social scientists*</a> lesson first.\n",
    "* **Audience**: Researchers and analysts from any disciplinary background. The materials are slightly tailored for social scientists through the use of social data.\n",
    "* **Learning outcomes**:\n",
    "    1. Understand the key steps and requirements for collecting data from web pages using computational methods.\n",
    "    2. Be able to use Python for requesting, parsing, extracting and saving data stored on a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*What is web-scraping?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a web-scraping?\n",
    "\n",
    "It is a computational technique for capturing information stored on a web page. \"Computational\" is the key word, as it is possible to perform this task manually, though that carries considerable disadvantages in terms of accuracy and labour resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Why would you want to \"scrape\" a web page?\n",
    "\n",
    "Web-scraping provides a computational means of quickly and accurately capturing data stored on web pages. Web pages can be an important source of publicly available information on phenomena of interest - for instance, they are used to store and disseminate files, text, photos, videos, tables etc. However, the data stored on websites are typically not structured or formatted for ease of use by researchers: for example, it may not be possible to perform a bulk download of all the files you need (think of needing the annual accounts of all registered companies in London for your research...), or the information may not even be held in a file and instead spread across paragraphs and tables throughout a web page (or worse, web pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is the general approach for scraping data from a web page?\n",
    "\n",
    "We begin by identifying a web page containing information we are interested in collecting. Then we need to **know** the following:\n",
    "1. The location (i.e., web address) where the web page can be accessed. For example, the UK Data Service homepage can be accessed via <a href=\"https://ukdataservice.ac.uk/\" target=_blank>https://ukdataservice.ac.uk/</a>.\n",
    "2. The location of the information we are interested in within the structure of the web page. This involves visually inspecting a web page's underlying code using a web browser.\n",
    "\n",
    "And **do** the following:\n",
    "3. Request the web page using its web address.\n",
    "4. Parse the structure of the web page so your programming language can work with its contents.\n",
    "5. Extract the information we are interested in.\n",
    "6. Write this information to a file for future use.\n",
    "\n",
    "For any programming task, it is useful to write out the steps needed to solve the problem: we call this *pseudo-code*, as it is captures the main tasks and the order in which they need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple web-scraping example\n",
    "\n",
    "Let's work through the steps in our general approach using a real web page, one that is designed for practicing web-scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Identifying the web address\n",
    "\n",
    "The web page we are interested in can be found at the following web address: <a href=\"https://httpbin.org/html\" target=_blank>https://httpbin.org/html</a>.\n",
    "\n",
    "You can click on the link to open the web page in your browser, though we could just use Python to view it in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://httpbin.org/html\", width=\"800\", height=\"650\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can see that the web page contains some text - this is an abstract from Herman Melville's classic novel *Moby Dick*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Locating information\n",
    "\n",
    "Our task is to extract the text on this web page. In order to do so, we need to understand where the text is located within the underlying *source code* of the web page. Web pages are written in a langauge called HyperText Markup Language (HTML). HTML describes the structure of a web page, and consists of a number of elements (e.g., paragraphs, tables, headers), with each element represented by a tag (e.g., `<p>`, `<table>`, `<h1>`). \n",
    "\n",
    "See <a href=\"https://www.w3schools.com/html/html_intro.asp\" target=_blank>https://www.w3schools.com/html/html_intro.asp</a> for more information on HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visually inspecting the underlying HTML code\n",
    "\n",
    "Therefore, what we need are the tags that identify the section of the web page where the text is stored. We can discover the tags by examining the *source code* (HTML) of the web page. This can be done using your web browser: for example, if you use use Firefox you can right-click on the web page and select *View Page Source* from the list of options. (Chrome: *View page source*; Safari: follow <a href=\"https://www.lifewire.com/view-html-source-in-safari-3469315\" target=_blank>these instructions</a>).\n",
    "\n",
    "The cell below shows the full HTML code for the web page."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "  </head>\n",
    "  <body>\n",
    "      <h1>Herman Melville - Moby-Dick</h1>\n",
    "\n",
    "      <div>\n",
    "        <p>\n",
    "          Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Perth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the foremast; being now almost incessantly invoked by the headsmen, and harpooneers, and bowsmen to do some little job for them; altering, or repairing, or new shaping their various weapons and boat furniture. Often he would be surrounded by an eager circle, all waiting to be served; holding boat-spades, pike-heads, harpoons, and lances, and jealously watching his every sooty movement, as he toiled. Nevertheless, this old man's was a patient hammer wielded by a patient arm. No murmur, no impatience, no petulance did come from him. Silent, slow, and solemn; bowing over still further his chronically broken back, he toiled away, as if toil were life itself, and the heavy beating of his hammer the heavy beating of his heart. And so it was.â€”Most miserable! A peculiar walk in this old man, a certain slight but painful appearing yawing in his gait, had at an early period of the voyage excited the curiosity of the mariners. And to the importunity of their persisted questionings he had finally given in; and so it came to pass that every one now knew the shameful story of his wretched fate. Belated, and not innocently, one bitter winter's midnight, on the road running between two country towns, the blacksmith half-stupidly felt the deadly numbness stealing over him, and sought refuge in a leaning, dilapidated barn. The issue was, the loss of the extremities of both feet. Out of this revelation, part by part, at last came out the four acts of the gladness, and the one long, and as yet uncatastrophied fifth act of the grief of his life's drama. He was an old man, who, at the age of nearly sixty, had postponedly encountered that thing in sorrow's technicals called ruin. He had been an artisan of famed excellence, and with plenty to do; owned a house and garden; embraced a youthful, daughter-like, loving wife, and three blithe, ruddy children; every Sunday went to a cheerful-looking church, planted in a grove. But one night, under cover of darkness, and further concealed in a most cunning disguisement, a desperate burglar slid into his happy home, and robbed them all of everything. And darker yet to tell, the blacksmith himself did ignorantly conduct this burglar into his family's heart. It was the Bottle Conjuror! Upon the opening of that fatal cork, forth flew the fiend, and shrivelled up his home. Now, for prudent, most wise, and economic reasons, the blacksmith's shop was in the basement of his dwelling, but with a separate entrance to it; so that always had the young and loving healthy wife listened with no unhappy nervousness, but with vigorous pleasure, to the stout ringing of her young-armed old husband's hammer; whose reverberations, muffled by passing through the floors and walls, came up to her, not unsweetly, in her nursery; and so, to stout Labor's iron lullaby, the blacksmith's infants were rocked to slumber. Oh, woe on woe! Oh, Death, why canst thou not sometimes be timely? Hadst thou taken this old blacksmith to thyself ere his full ruin came upon him, then had the young widow had a delicious grief, and her orphans a truly venerable, legendary sire to dream of in their after years; and all of them a care-killing competency.\n",
    "        </p>\n",
    "      </div>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the HTML code above, we can see multiple tags identifying different elements on the web page: there is a set of `<h1></h1>` tags representing the page title, a set of `<div></div>` tags representing a section, and a set of `<p></p>` tags representing the paragraph containing the text we are interested. (There are also some metadata tags outwith the `<body></body>` tags that we do not need to concern ourselves with)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Requesting the web page\n",
    "\n",
    "Now that we possess the necessary information, let's begin the process of scraping the web page. There is a preliminary step, which is setting up Python with the modules it needs to perform the web-scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import os # module for navigating your machine (e.g., file directories)\n",
    "import requests # module for requesting urls\n",
    "from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Modules are additional techniques or functions that are not present when you launch Python. Some do not even come with Python when you download it and must be installed on your machine separately - think of using `ssc install <package>` in Stata, or `install.packages(<package>)` in R. For now just understand that many useful modules need to be imported every time you start a new Python session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, let's implement the process of scraping the page. First, we need to request the web page using Python; this is analogous to opening a web browser and entering the web address manually. We refer to a page's location on the internet as its web address or Uniform Resource Locator (URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the URL where the web page can be accessed\n",
    "\n",
    "url = \"https://httpbin.org/html\"\n",
    "\n",
    "# Request the web page\n",
    "\n",
    "response = requests.get(url, allow_redirects=True) # request the url\n",
    "response.status_code # check if page was requested successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Good, we get a status code of *200*, which means the request was successful. A status code in *400s* or *500s* represent an unsuccessful attempt at requesting a web page (see <a href=\"https://www.textbook.ds100.org/ch/07/web_http.html\" target=_blank>Lau, Gonzalez and Nolan</a> for a succinct description of different types of response status codes).\n",
    "\n",
    "Let's unpack the code a bit. First, we define a variable (also known as an 'object' in Python) called `url` that contains the web address of the page we want to request. Next, we use the `get()` method of the `requests` module to request the web page, and in the same line of code, we store the results of the request in a variable called `response`. Finally, we check whether the request was successful by calling on the `status_code` attribute of the `response` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also view the metadata associated with our request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You may be wondering exactly what it is we requested: if you were to type the URL (https://httpbin.org/html) into your browser and hit `enter`, the web page should appear on your screen. This is not the case when we request the URL through Python but rest assured, we have successfully requested the web page. To see the content of our request, we can examine the `text` attribute of the `response` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This shows us the underlying code (HTML) of the web page we requested. It should be obvious that in its current form, the result of this request will be difficult to work with. This is where the `BeautifulSoup` module comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parsing the web page\n",
    "\n",
    "Now it's time to identify and understand the structure of the web page we requested. We do this by converting the content contained in the `response.text` attribute into a `BeautifulSoup` variable. `BeautifulSoup` is a Python module that provides a systematic way of navigating the elements of a web page and extracting its contents. Let's see how it works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the contents of the webpage from the response\n",
    "\n",
    "soup_response = soup(response.text, \"html.parser\") # Parse the text as a Beautiful Soup object\n",
    "soup_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice how the hierarchical structure of the web page is now recognised by Python? Not only that, `BeautifulSoup` provides some methods for accessing the tags contained in the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting information\n",
    "\n",
    "Now that we have parsed the web page, we can use Python to navigate and extract the information of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "paragraph = soup_response.find(\"p\")\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We used the `find()` method on the `soup_response` variable to capture the set of `<p></p>` tags on the page. Remember, we used our visual inspection of the source code to identify that the text we needed was contained within a set of `<p></p>` tags, and that there was only one set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're near the end of the scrape: we just need to extract the text from within the tags like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = paragraph.text\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving results from the scrape\n",
    "\n",
    "Let's conclude by saving the scraped data to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a file to store the data\n",
    "\n",
    "outfile = \"./moby-dick-scraped-data.txt\" # location and name of file\n",
    "\n",
    "# Open the file and write (save) the data to it\n",
    "\n",
    "with open(outfile, \"w\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we know this worked? The simplest way is to check whether a) the file was created, and b) the results were written to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check presence of file in current folder\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Open file and read (import) its contents\n",
    "\n",
    "with open(outfile, \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And Voila, we have successfully scraped a web page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A social science example\n",
    "\n",
    "While the above example is good for learning the basics, scraping research-relevant data from a web page is a little more difficult:\n",
    "* Data may be spread throughout a web page (or across multiple pages).\n",
    "* There may many tags with similar data that need to be filtered in order to get to the information you need.\n",
    "* Any many other potential issues.\n",
    "\n",
    "Let's look a social data example to get a better sense of what web-scraping for research involves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Collecting charity data\n",
    "\n",
    "For one of us (Diarmuid), web-scraping provides a means of collecting data that cannot be accessed any other way (other than manually copying-and-pasting from each charity's web page...). In particular, we are interested in collecting data about which policies a charity reportedly has in place. This information is interesting as it can be linked to real organisational outcomes (e.g., is there a correlation between not having a risk policy and going out of business?).\n",
    "\n",
    "Once again we'll work through the key steps in our general approach, this time a bit quicker and with less narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import requests # module for requesting urls\n",
    "import os # module for performing operating system tasks\n",
    "import pandas as pd # module for working with datasets\n",
    "from IPython.display import IFrame # module for embedding web pages, documents etc\n",
    "from bs4 import BeautifulSoup as soup # module for parsing web pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Identifying the web address\n",
    "\n",
    "We're going to use the Charity Commission for England and Wales' website to capture policy data: <a href=\"https://beta.charitycommission.gov.uk/\" target=_blank>https://beta.charitycommission.gov.uk/</a>\n",
    "\n",
    "We're going to focus on just one charity for now - Oxfam; therefore the web address looks like this: <a href=\"https://beta.charitycommission.gov.uk/charity-details/?regId=202918&subId=0\" target=_blank>https://beta.charitycommission.gov.uk/charity-details/?regId=202918&subId=0</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://beta.charitycommission.gov.uk/charity-details/?regId=202918&subId=0\", width=\"800\", height=\"650\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Locating information\n",
    "\n",
    "Policy data is located in the *Documents* tab under a heading called *Policies*, which in terms of the source code is here:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"pcg-charity-details__block col-lg-6\">\n",
    "    <h3>Policies</h3>\n",
    "        <span>Risk management</span>\n",
    "        <br />\n",
    "        <span>Investment</span>\n",
    "        <br />\n",
    "        <span>Safeguarding vulnerable beneficiaries</span>\n",
    "        <br />\n",
    "        <span>Conflicts of interest</span>\n",
    "        <br />\n",
    "        <span>Volunteer management</span>\n",
    "        <br />\n",
    "        <span>Complaints handling</span>\n",
    "        <br />\n",
    "        <span>Paying staff</span>\n",
    "        <br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Requesting the web page\n",
    "\n",
    "Now that we possess the necessary information, let's begin the process of scraping the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://beta.charitycommission.gov.uk/charity-details/?regId=202918&subId=0\"\n",
    "\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parsing the web page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup_response = soup(response.text, \"html.parser\")\n",
    "# soup_response.body # view HTML code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting information\n",
    "\n",
    "The policies are contained within a set of `<div></div>` tags where the *class* attribute equals \"pcg-charity-details__block col-lg-6\". There are multiple sets of tags with this id, therefore we need to use the `find_all()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sections = soup_response.find_all(\"div\", class_=\"pcg-charity-details__block col-lg-6\")\n",
    "len(sections) # view how many sets of tags are returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Multiple sets of tags are returned, therefore how can we identify the correct set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "searchterm = \"Policies\" # search term identifying section containing list of policies\n",
    "\n",
    "for section in sections: # for each section contained in the sections list:\n",
    "    if searchterm in str(section): # if the search term exists in the section\n",
    "        policy_location = sections.index(section) # store the list location of the correct section\n",
    "        print(policy_location) # view the location of the policies in the list (i.e, is it the first element in the list?)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "policy_section = sections[policy_location] # create a new variable containing the correct section\n",
    "policy_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's unpick the logic of the code above:\n",
    "1. We know the list of policies is contained in a section (`<div>`) where *class_=\"pcg-charity-details__block col-lg-6\"*.\n",
    "2. We find all sections where the _class_ attribute equals \"pcg-charity-details__block col-lg-6\", and navigate to the correct one by evaluating whether it contains a relevant piece of text (\"Policies\"). This process revealed that the list of policies was contained in the sixth section (remember: lists begin at position 0, so 5 identifies the sixth element of a list). If we knew that the list of beneficiaries was always contained in the fifth section we wouldn't need the use of a search term, but this way is more robust to deviations in the structure and content of each charity's web page.\n",
    "\n",
    "Now that we have the correct set of `<div></div>` tags, we need to extract the policy data from with the `<span></span>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "policy_list = [] # define a blank list for storing the policy data\n",
    "charity_name = \"Oxfam\" # define a variable for storing the charity's name\n",
    "\n",
    "for tag in policy_section.find_all(\"span\"): # for each set of span tags in the policy section\n",
    "    policy = tag.text # extract the text from the tag\n",
    "    observation = [charity_name, policy] # combine charity name and a policy\n",
    "    policy_list.append(observation) # append the charity name and policy to the blank list\n",
    "    \n",
    "policy_list # view list of policies for the charity (long format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Again, let's unpack the code above:\n",
    "1. We define a variable called `policy_list` which will store the extracted text; at this point the list is empty. We also define a variable for storing the charity's name (`charity_name`).\n",
    "2. Then, for each set of `<span></span>` tags in the `policy_section` variable, we extract the text from within the tags. We also define a variable called `observation` with stores a list of values: a charity's name and a given policy; finally we append the information to the empty list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving results from the scrape\n",
    "\n",
    "Let's conclude by saving the scraped data to a file for future use. We'll make use of the excellent `pandas` module (shortened to `pd`) for converting the extracted text to a dataset prior to writing to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "policy_data = pd.DataFrame(list(policy_list), columns=[\"charity_name\", \"policy\"])\n",
    "policy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, we can save the data set to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "outfile = \"./oxfam_policies.csv\"\n",
    "\n",
    "policy_data.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(outfile, encoding=\"ISO-8859-1\", index_col=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import modules into Python to support your work. Python does come with some methods and functions that are ready to use straight away, but for computational social science tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to request and parse web pages**. You can use Python to request a web page, and the `BeautifulSoup` module to parse its contents.\n",
    "* **How to read and write data**. You can save the results of your scrape to a file for future use.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Web-scraping is a simple yet powerful computational method for collecting data of value for social science research. It provides a relatively gentle introduction to using programming languages, also. However, \"with great power comes great responsibility\" (sorry). Web-scraping takes you into the realm of data protection, website Terms of Service (ToS), and many murky ethical issues. Wielded sensibly and sensitively, web-scraping is a valuable and exciting social science research method. \n",
    "\n",
    "Good luck on your data-driven travels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Bibliography\n",
    "\n",
    "Barba, Lorena A. et al. (2019). *Teaching and Learning with Jupyter*. <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>https://jupyter4edu.github.io/jupyter-edu-book/</a>.\n",
    "\n",
    "Lau, S., Gonzalez, J., & Nolan, D. (n.d.). *Principles and Techniques of Data Science*. https://www.textbook.ds100.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading and resources\n",
    "\n",
    "We hope this brief lession has whetted your appetite for learning more about web-scraping and Python programming in general. There are some fantastic learning materials available to you, many of them free. We highly recommend the materials referenced in the Bibliography.\n",
    "\n",
    "In addition, you may find the following resources useful:\n",
    "* <a href=\"https://github.com/UKDataServiceOpen/web-scraping\" target=_blank>**Web-scraping for Social Science Research**</a> - a free UK Data Service training series on web-scraping, with three webinars and lots of detailed coding examples.\n",
    "* <a href=\"https://automatetheboringstuff.com/\" target=_blank>**Automate the Boring Stuff with Python**</a> - a free ebook covering lots of interesting, practical uses of Python. Chapter 12 covers web-scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
